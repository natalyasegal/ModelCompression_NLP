{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lSLPMz07v78"
      },
      "source": [
        "# Model Compression - Emotions Dataset - Tranformer based models\n",
        "\n",
        "This notebook provides model compression\n",
        "\n",
        "## Compression tecniques:\n",
        "1. **Pruning** - model layers pruning\n",
        "2. **Quantization** - unint8 quantization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxaJXA_B7v7-"
      },
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMvgSBRR7v7_",
        "outputId": "73fe0587-6ffd-4430-edba-16a2f0597e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries imported successfully\n",
            "Pandas version: 2.2.2\n",
            "NumPy version: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"✅ Libraries imported successfully\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_JxxNd17v8A"
      },
      "source": [
        "## 2. Load Training Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgRHy0qK8UH0"
      },
      "source": [
        "### mount and unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Kd7-jZ803h",
        "outputId": "6befecdb-dded-4a77-eefd-8617ff2c1174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRwHUA3g87KQ",
        "outputId": "c3f2bb63-5e7b-4d65-8653-a3419de0cb56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot open 'gdrive/MyDrive/__PHd_2025/courses/2026a/NLP/data/train.gsheet' for reading: Operation not supported\n",
            "Archive:  data/data.zip\n",
            "  inflating: validation.csv          \n",
            "  inflating: train.csv               \n",
            "data.zip  train.csv  validation.csv\n"
          ]
        }
      ],
      "source": [
        "!cp -r gdrive/MyDrive/__PHd_2025/courses/2026a/NLP/data .\n",
        "!unzip data/data.zip\n",
        "!mv *csv data/.\n",
        "!ls data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC3vacDl8XcG"
      },
      "source": [
        "### Load and take a look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDkknuTL7v8A",
        "outputId": "adb51eae-0543-4b59-a821-10c6a1ffcfb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: train: (16000, 2) val: (2000, 2)\n",
            "Columns: train: ['text', 'label'], val: ['text', 'label']\n",
            "Train labels distribution:\n",
            "label\n",
            "1    5362\n",
            "0    4666\n",
            "3    2159\n",
            "4    1937\n",
            "2    1304\n",
            "5     572\n",
            "Name: count, dtype: int64\n",
            "Validation labels distribution:\n",
            "label\n",
            "1    704\n",
            "0    550\n",
            "3    275\n",
            "4    212\n",
            "2    178\n",
            "5     81\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "df_t = pd.read_csv('./data/train.csv')\n",
        "df_v = pd.read_csv('./data/validation.csv')\n",
        "\n",
        "print(f\"Dataset shape: train: {df_t.shape} val: {df_v.shape}\")\n",
        "print(f\"Columns: train: {list(df_t.columns)}, val: {list(df_v.columns)}\")\n",
        "\n",
        "print(\"Train labels distribution:\")\n",
        "print(df_t.label.value_counts())\n",
        "print(\"Validation labels distribution:\")\n",
        "print(df_v.label.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code from repo"
      ],
      "metadata": {
        "id": "7bUUsgOOkYJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ModelCompression_NLP/ # remove previous version\n",
        "\n",
        "#if you are clonning a public version, use:\n",
        "!git clone https://github.com/natalyasegal/ModelCompression_NLP.git\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/ModelCompression_NLP')   # add package root to Python path\n",
        "\n",
        "#from utils.swap import swap_categories\n",
        "from compress.compress import parse_int_list, supersample_train_df, train_one_model, apply_global_magnitude_pruning, linear_sparsity, CSVDataset, make_training_args, compute_metrics, WeightedLossTrainer, dynamic_int8_quantize, plot_losses_from_trainer, model_disk_size_mb, TrainResult\n",
        "\n",
        "from eval.eval import evaluate_all_versions_from_outputs"
      ],
      "metadata": {
        "id": "4XLRRCAdkaVh",
        "outputId": "6de74af2-bea3-42d3-c854-f5e80fb99658",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ModelCompression_NLP'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 14 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14/14), 70.29 KiB | 35.15 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compress Roberta and compare"
      ],
      "metadata": {
        "id": "OZNATZfbkcgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    cohen_kappa_score,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--train_csv\", type=str, default=\"data/train.csv\")\n",
        "    p.add_argument(\"--text_col\", type=str, default=\"text\")\n",
        "    p.add_argument(\"--label_col\", type=str, default=\"label\")\n",
        "    p.add_argument(\"--out_dir\", type=str, default=\"./outputs_part2\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "    p.add_argument(\"--val_ratio\", type=float, default=0.15)\n",
        "    p.add_argument(\"--epochs\", type=int, default=10)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=16)\n",
        "    p.add_argument(\"--lr\", type=float, default=2e-5)\n",
        "    p.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
        "    p.add_argument(\"--max_length\", type=int, default=128)\n",
        "    p.add_argument(\"--patience\", type=int, default=2)\n",
        "    p.add_argument(\"--no_weighted_loss\", action=\"store_true\")\n",
        "    p.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    p.add_argument(\"--show_plots\", action=\"store_true\")\n",
        "\n",
        "    p.add_argument(\n",
        "        \"--supersample_factors\",\n",
        "        type=str,\n",
        "        default=\"1,1,4,2,3,8\",\n",
        "        help='e.g. \"1,1,1,1,1,1\" (no oversample) or \"1,1,1,1,1,2\" (double class 5)'\n",
        "    )\n",
        "\n",
        "    # Model Compression\n",
        "    p.add_argument(\"--prune_amount\", type=float, default=0.35)\n",
        "    p.add_argument(\"--prune_recover_epochs\", type=int, default=1)\n",
        "\n",
        "    args, _ = p.parse_known_args(argv)\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "\n",
        "    df = pd.read_csv(args.train_csv)\n",
        "    num_labels = int(df[args.label_col].nunique())\n",
        "\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        test_size=args.val_ratio,\n",
        "        random_state=args.seed,\n",
        "        stratify=df[args.label_col],\n",
        "    )\n",
        "\n",
        "    # Apply supersampling ONLY to training split\n",
        "    if args.supersample_factors is not None:\n",
        "        factors = parse_int_list(args.supersample_factors)\n",
        "        train_df = supersample_train_df(train_df, args.label_col, factors=factors, seed=args.seed)\n",
        "        print(\"Applied supersampling factors:\", factors)\n",
        "        print(\"New train label counts:\\n\", train_df[args.label_col].value_counts().sort_index())\n",
        "\n",
        "    candidates = [\n",
        "        \"roberta-base\",\n",
        "        \"cardiffnlp/twitter-roberta-base\",\n",
        "        \"distilroberta-base\"]\n",
        "\n",
        "    results: List[TrainResult] = []\n",
        "    for m in candidates:\n",
        "        print(f\"\\n=== Training {m} ===\")\n",
        "        r = train_one_model(\n",
        "            model_name=m,\n",
        "            train_df=train_df,\n",
        "            val_df=val_df,\n",
        "            text_col=args.text_col,\n",
        "            label_col=args.label_col,\n",
        "            num_labels=num_labels,\n",
        "            out_root=args.out_dir,\n",
        "            seed=args.seed,\n",
        "            epochs=args.epochs,\n",
        "            batch_size=args.batch_size,\n",
        "            lr=args.lr,\n",
        "            weight_decay=args.weight_decay,\n",
        "            max_length=args.max_length,\n",
        "            patience=args.patience,\n",
        "            use_weighted_loss=(not args.no_weighted_loss),\n",
        "            device=args.device,\n",
        "            show_plots=args.show_plots,\n",
        "        )\n",
        "        results.append(r)\n",
        "        print(\"Saved:\", r.saved_dir)\n",
        "        print(\"Loss plot:\", r.loss_plot_path)\n",
        "        print(\"Size(MB):\", round(r.size_mb, 2))\n",
        "        print(\"Eval macro-F1:\", round(r.best_metric, 6))\n",
        "\n",
        "    best = max(results, key=lambda x: x.best_metric)\n",
        "    print(\"\\n=== BEST MODEL ===\")\n",
        "    print(\"Best:\", best.model_name)\n",
        "    print(\"Dir :\", best.saved_dir)\n",
        "\n",
        "    # -------------------------\n",
        "    # COMPRESSION 1: PRUNING + RECOVERY FINETUNE\n",
        "    # -------------------------\n",
        "    print(\"\\n=== COMPRESSION 1: PRUNING + RECOVERY FINETUNE ===\")\n",
        "    best_tok = AutoTokenizer.from_pretrained(best.saved_dir, use_fast=True)\n",
        "    best_model = AutoModelForSequenceClassification.from_pretrained(best.saved_dir)\n",
        "\n",
        "    pruned_model = apply_global_magnitude_pruning(best_model, amount=args.prune_amount)\n",
        "    sp = linear_sparsity(pruned_model)\n",
        "    print(f\"Pruned linear sparsity: {sp:.3f}\")\n",
        "\n",
        "    pruned_dir = os.path.join(args.out_dir, \"BEST_PRUNED\")\n",
        "    os.makedirs(pruned_dir, exist_ok=True)\n",
        "\n",
        "    train_ds = CSVDataset(train_df, best_tok, args.text_col, args.label_col, max_length=args.max_length)\n",
        "    val_ds = CSVDataset(val_df, best_tok, args.text_col, args.label_col, max_length=args.max_length)\n",
        "    collator = DataCollatorWithPadding(best_tok)\n",
        "\n",
        "    class_counts = train_df[args.label_col].value_counts().sort_index().reindex(range(num_labels), fill_value=0).values\n",
        "    inv = 1.0 / torch.tensor(class_counts + 1e-9, dtype=torch.float)\n",
        "    class_weights = inv / inv.sum()\n",
        "\n",
        "    pruned_args = make_training_args(\n",
        "        output_dir=pruned_dir,\n",
        "        num_train_epochs=args.prune_recover_epochs,\n",
        "        learning_rate=min(args.lr, 1e-5),\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        greater_is_better=True,\n",
        "        logging_strategy=\"epoch\",\n",
        "        report_to=\"none\",\n",
        "        seed=args.seed,\n",
        "        fp16=(\"cuda\" in args.device and torch.cuda.is_available()),\n",
        "    )\n",
        "\n",
        "    if args.no_weighted_loss:\n",
        "        pruned_trainer = Trainer(\n",
        "            model=pruned_model,\n",
        "            args=pruned_args,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=val_ds,\n",
        "            tokenizer=best_tok,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "    else:\n",
        "        pruned_trainer = WeightedLossTrainer(\n",
        "            class_weights=class_weights,\n",
        "            model=pruned_model,\n",
        "            args=pruned_args,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=val_ds,\n",
        "            tokenizer=best_tok,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "    pruned_trainer.train()\n",
        "    pruned_metrics = pruned_trainer.evaluate()\n",
        "\n",
        "    pruned_trainer.save_model(pruned_dir)\n",
        "    best_tok.save_pretrained(pruned_dir)\n",
        "\n",
        "    pruned_loss_png = os.path.join(pruned_dir, \"loss_curve.png\")\n",
        "    plot_losses_from_trainer(pruned_trainer, pruned_loss_png, show=args.show_plots)\n",
        "\n",
        "    pruned_size = model_disk_size_mb(pruned_trainer.model)\n",
        "    print(\"Pruned dir:\", pruned_dir)\n",
        "    print(\"Pruned size(MB):\", round(pruned_size, 2))\n",
        "    print(\"Pruned metrics:\", {k: round(float(v), 6) for k, v in pruned_metrics.items() if isinstance(v, (int, float, np.floating))})\n",
        "\n",
        "    # -------------------------\n",
        "    # COMPRESSION 2: DYNAMIC INT8 QUANTIZATION (CPU)\n",
        "    # -------------------------\n",
        "    print(\"\\n=== COMPRESSION 2: DYNAMIC INT8 QUANTIZATION (CPU) ===\")\n",
        "    best_model_clean = AutoModelForSequenceClassification.from_pretrained(best.saved_dir)\n",
        "    qmodel = dynamic_int8_quantize(best_model_clean)\n",
        "\n",
        "    texts = val_df[args.text_col].astype(str).tolist()\n",
        "    labels = val_df[args.label_col].astype(int).to_numpy()\n",
        "\n",
        "    preds_all = []\n",
        "    bs = 32\n",
        "    for i in range(0, len(texts), bs):\n",
        "        batch = texts[i:i + bs]\n",
        "        enc = best_tok(batch, padding=True, truncation=True, max_length=args.max_length, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            logits = qmodel(**enc).logits\n",
        "        preds_all.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds_all, axis=0)\n",
        "\n",
        "    q_metrics = {\n",
        "        \"macro_f1\": float(f1_score(labels, preds, average=\"macro\")),\n",
        "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
        "        \"balanced_accuracy\": float(balanced_accuracy_score(labels, preds)),\n",
        "        \"kappa\": float(cohen_kappa_score(labels, preds)),\n",
        "    }\n",
        "    for c in range(num_labels):\n",
        "        mask = labels == c\n",
        "        q_metrics[f\"acc_c{c}\"] = float((preds[mask] == labels[mask]).mean()) if mask.sum() else float(\"nan\")\n",
        "\n",
        "    q_size = model_disk_size_mb(qmodel)\n",
        "\n",
        "    quant_dir = os.path.join(args.out_dir, \"BEST_INT8_CPU\")\n",
        "    os.makedirs(quant_dir, exist_ok=True)\n",
        "    torch.save(qmodel.state_dict(), os.path.join(quant_dir, \"pytorch_model.bin\"))\n",
        "    best_tok.save_pretrained(quant_dir)\n",
        "    with open(os.path.join(quant_dir, \"meta.json\"), \"w\") as f:\n",
        "        json.dump({\"base_model_dir\": best.saved_dir, \"note\": \"dynamic int8 quantized (CPU) Linear layers\"}, f, indent=2)\n",
        "\n",
        "    print(\"Quant dir:\", quant_dir)\n",
        "    print(\"Quant size(MB):\", round(q_size, 2))\n",
        "    print(\"Quant metrics:\", {k: round(float(v), 6) for k, v in q_metrics.items()})\n",
        "\n",
        "    # Summary JSON\n",
        "    summary_path = os.path.join(args.out_dir, \"summary.json\")\n",
        "    summary = {\n",
        "        \"best_model\": best.model_name,\n",
        "        \"best_dir\": best.saved_dir,\n",
        "        \"best_metrics\": best.metrics,\n",
        "        \"pruned_dir\": pruned_dir,\n",
        "        \"pruned_metrics\": {k: float(v) for k, v in pruned_metrics.items() if isinstance(v, (int, float, np.floating))},\n",
        "        \"pruned_sparsity\": sp,\n",
        "        \"quant_dir\": quant_dir,\n",
        "        \"quant_metrics\": q_metrics,\n",
        "    }\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(\"\\n=== DONE ===\")\n",
        "    print(\"Summary:\", summary_path)\n",
        "    print(\"All outputs in:\", args.out_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "FkuPwiWdke-0",
        "outputId": "1a897b8e-ea14-413c-a93a-d80d3ae4ca56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied supersampling factors: [1, 1, 4, 2, 3, 8]\n",
            "New train label counts:\n",
            " label\n",
            "0    3966\n",
            "1    4558\n",
            "2    4432\n",
            "3    3670\n",
            "4    4941\n",
            "5    3888\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Training roberta-base ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7955' max='15910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 7955/15910 06:23 < 06:24, 20.71 it/s, Epoch 5/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Balanced Accuracy</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>Acc C0</th>\n",
              "      <th>Acc C1</th>\n",
              "      <th>Acc C2</th>\n",
              "      <th>Acc C3</th>\n",
              "      <th>Acc C4</th>\n",
              "      <th>Acc C5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.399800</td>\n",
              "      <td>0.228453</td>\n",
              "      <td>0.900251</td>\n",
              "      <td>0.929583</td>\n",
              "      <td>0.941690</td>\n",
              "      <td>0.908847</td>\n",
              "      <td>0.958571</td>\n",
              "      <td>0.896766</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.947531</td>\n",
              "      <td>0.879310</td>\n",
              "      <td>0.988372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.163500</td>\n",
              "      <td>0.244094</td>\n",
              "      <td>0.899826</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.930583</td>\n",
              "      <td>0.909321</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.894279</td>\n",
              "      <td>0.989796</td>\n",
              "      <td>0.966049</td>\n",
              "      <td>0.889655</td>\n",
              "      <td>0.883721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.275560</td>\n",
              "      <td>0.901075</td>\n",
              "      <td>0.932500</td>\n",
              "      <td>0.927239</td>\n",
              "      <td>0.912443</td>\n",
              "      <td>0.962857</td>\n",
              "      <td>0.902985</td>\n",
              "      <td>0.989796</td>\n",
              "      <td>0.938272</td>\n",
              "      <td>0.920690</td>\n",
              "      <td>0.848837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.090400</td>\n",
              "      <td>0.344940</td>\n",
              "      <td>0.895528</td>\n",
              "      <td>0.929167</td>\n",
              "      <td>0.907315</td>\n",
              "      <td>0.907716</td>\n",
              "      <td>0.955714</td>\n",
              "      <td>0.914179</td>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.953704</td>\n",
              "      <td>0.934483</td>\n",
              "      <td>0.767442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.428163</td>\n",
              "      <td>0.898781</td>\n",
              "      <td>0.930417</td>\n",
              "      <td>0.917030</td>\n",
              "      <td>0.909465</td>\n",
              "      <td>0.948571</td>\n",
              "      <td>0.920398</td>\n",
              "      <td>0.913265</td>\n",
              "      <td>0.941358</td>\n",
              "      <td>0.941379</td>\n",
              "      <td>0.837209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: ./outputs_part2/roberta-base\n",
            "Loss plot: ./outputs_part2/roberta-base/loss_curve.png\n",
            "Size(MB): 475.58\n",
            "Eval macro-F1: 0.901075\n",
            "\n",
            "=== Training cardiffnlp/twitter-roberta-base ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='15910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    7/15910 00:00 < 15:52, 16.70 it/s, Epoch 0.00/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddhqlTIz_OZm"
      },
      "source": [
        "### Prev run\n",
        "- used those results for the report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H86OUNQZRVuD",
        "outputId": "3ade6158-314f-44dc-ce4f-d03ed6bb1f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applied supersampling factors: [1, 1, 4, 2, 3, 8]\n",
            "New train label counts:\n",
            " label\n",
            "0    3966\n",
            "1    4558\n",
            "2    4432\n",
            "3    3670\n",
            "4    4941\n",
            "5    3888\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Training roberta-base ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7955' max='15910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 7955/15910 06:22 < 06:22, 20.81 it/s, Epoch 5/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Balanced Accuracy</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>Acc C0</th>\n",
              "      <th>Acc C1</th>\n",
              "      <th>Acc C2</th>\n",
              "      <th>Acc C3</th>\n",
              "      <th>Acc C4</th>\n",
              "      <th>Acc C5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.399800</td>\n",
              "      <td>0.228453</td>\n",
              "      <td>0.900251</td>\n",
              "      <td>0.929583</td>\n",
              "      <td>0.941690</td>\n",
              "      <td>0.908847</td>\n",
              "      <td>0.958571</td>\n",
              "      <td>0.896766</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.947531</td>\n",
              "      <td>0.879310</td>\n",
              "      <td>0.988372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.163500</td>\n",
              "      <td>0.244094</td>\n",
              "      <td>0.899826</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.930583</td>\n",
              "      <td>0.909321</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.894279</td>\n",
              "      <td>0.989796</td>\n",
              "      <td>0.966049</td>\n",
              "      <td>0.889655</td>\n",
              "      <td>0.883721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.275560</td>\n",
              "      <td>0.901075</td>\n",
              "      <td>0.932500</td>\n",
              "      <td>0.927239</td>\n",
              "      <td>0.912443</td>\n",
              "      <td>0.962857</td>\n",
              "      <td>0.902985</td>\n",
              "      <td>0.989796</td>\n",
              "      <td>0.938272</td>\n",
              "      <td>0.920690</td>\n",
              "      <td>0.848837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.090400</td>\n",
              "      <td>0.344940</td>\n",
              "      <td>0.895528</td>\n",
              "      <td>0.929167</td>\n",
              "      <td>0.907315</td>\n",
              "      <td>0.907716</td>\n",
              "      <td>0.955714</td>\n",
              "      <td>0.914179</td>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.953704</td>\n",
              "      <td>0.934483</td>\n",
              "      <td>0.767442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.428163</td>\n",
              "      <td>0.898781</td>\n",
              "      <td>0.930417</td>\n",
              "      <td>0.917030</td>\n",
              "      <td>0.909465</td>\n",
              "      <td>0.948571</td>\n",
              "      <td>0.920398</td>\n",
              "      <td>0.913265</td>\n",
              "      <td>0.941358</td>\n",
              "      <td>0.941379</td>\n",
              "      <td>0.837209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: ./outputs_part2/roberta-base\n",
            "Loss plot: ./outputs_part2/roberta-base/loss_curve.png\n",
            "Size(MB): 475.58\n",
            "Eval macro-F1: 0.901075\n",
            "\n",
            "=== Training cardiffnlp/twitter-roberta-base ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6364' max='15910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6364/15910 05:02 < 07:33, 21.05 it/s, Epoch 4/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Balanced Accuracy</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>Acc C0</th>\n",
              "      <th>Acc C1</th>\n",
              "      <th>Acc C2</th>\n",
              "      <th>Acc C3</th>\n",
              "      <th>Acc C4</th>\n",
              "      <th>Acc C5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.355800</td>\n",
              "      <td>0.227766</td>\n",
              "      <td>0.899978</td>\n",
              "      <td>0.928750</td>\n",
              "      <td>0.941152</td>\n",
              "      <td>0.907801</td>\n",
              "      <td>0.954286</td>\n",
              "      <td>0.899254</td>\n",
              "      <td>0.984694</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.875862</td>\n",
              "      <td>0.988372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.254911</td>\n",
              "      <td>0.902477</td>\n",
              "      <td>0.930833</td>\n",
              "      <td>0.923940</td>\n",
              "      <td>0.910251</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.896766</td>\n",
              "      <td>0.989796</td>\n",
              "      <td>0.978395</td>\n",
              "      <td>0.893103</td>\n",
              "      <td>0.825581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.088800</td>\n",
              "      <td>0.325324</td>\n",
              "      <td>0.897504</td>\n",
              "      <td>0.928333</td>\n",
              "      <td>0.919407</td>\n",
              "      <td>0.906758</td>\n",
              "      <td>0.968571</td>\n",
              "      <td>0.909204</td>\n",
              "      <td>0.954082</td>\n",
              "      <td>0.922840</td>\n",
              "      <td>0.889655</td>\n",
              "      <td>0.872093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.059700</td>\n",
              "      <td>0.383849</td>\n",
              "      <td>0.894258</td>\n",
              "      <td>0.932083</td>\n",
              "      <td>0.901161</td>\n",
              "      <td>0.911325</td>\n",
              "      <td>0.967143</td>\n",
              "      <td>0.926617</td>\n",
              "      <td>0.877551</td>\n",
              "      <td>0.966049</td>\n",
              "      <td>0.913793</td>\n",
              "      <td>0.755814</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: ./outputs_part2/cardiffnlp__twitter-roberta-base\n",
            "Loss plot: ./outputs_part2/cardiffnlp__twitter-roberta-base/loss_curve.png\n",
            "Size(MB): 475.58\n",
            "Eval macro-F1: 0.902477\n",
            "\n",
            "=== Training distilroberta-base ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6364' max='15910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6364/15910 03:03 < 04:34, 34.73 it/s, Epoch 4/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Balanced Accuracy</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>Acc C0</th>\n",
              "      <th>Acc C1</th>\n",
              "      <th>Acc C2</th>\n",
              "      <th>Acc C3</th>\n",
              "      <th>Acc C4</th>\n",
              "      <th>Acc C5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.397400</td>\n",
              "      <td>0.236061</td>\n",
              "      <td>0.901322</td>\n",
              "      <td>0.928750</td>\n",
              "      <td>0.942218</td>\n",
              "      <td>0.907751</td>\n",
              "      <td>0.957143</td>\n",
              "      <td>0.894279</td>\n",
              "      <td>0.974490</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.855172</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.151100</td>\n",
              "      <td>0.233079</td>\n",
              "      <td>0.906029</td>\n",
              "      <td>0.932500</td>\n",
              "      <td>0.933400</td>\n",
              "      <td>0.912438</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.899254</td>\n",
              "      <td>0.974490</td>\n",
              "      <td>0.950617</td>\n",
              "      <td>0.920690</td>\n",
              "      <td>0.895349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.105400</td>\n",
              "      <td>0.248510</td>\n",
              "      <td>0.905696</td>\n",
              "      <td>0.935417</td>\n",
              "      <td>0.930716</td>\n",
              "      <td>0.916158</td>\n",
              "      <td>0.968571</td>\n",
              "      <td>0.907960</td>\n",
              "      <td>0.984694</td>\n",
              "      <td>0.947531</td>\n",
              "      <td>0.903448</td>\n",
              "      <td>0.872093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.077400</td>\n",
              "      <td>0.295665</td>\n",
              "      <td>0.898656</td>\n",
              "      <td>0.929583</td>\n",
              "      <td>0.918726</td>\n",
              "      <td>0.908429</td>\n",
              "      <td>0.957143</td>\n",
              "      <td>0.911692</td>\n",
              "      <td>0.943878</td>\n",
              "      <td>0.978395</td>\n",
              "      <td>0.872414</td>\n",
              "      <td>0.848837</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: ./outputs_part2/distilroberta-base\n",
            "Loss plot: ./outputs_part2/distilroberta-base/loss_curve.png\n",
            "Size(MB): 313.32\n",
            "Eval macro-F1: 0.906029\n",
            "\n",
            "=== BEST MODEL ===\n",
            "Best: distilroberta-base\n",
            "Dir : ./outputs_part2/distilroberta-base\n",
            "\n",
            "=== COMPRESSION 1: PRUNING + RECOVERY FINETUNE ===\n",
            "Pruned linear sparsity: 0.350\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1532' max='1591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1532/1591 00:41 < 00:01, 37.08 it/s, Epoch 0.96/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1591' max='1591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1591/1591 00:45, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Balanced Accuracy</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>Acc C0</th>\n",
              "      <th>Acc C1</th>\n",
              "      <th>Acc C2</th>\n",
              "      <th>Acc C3</th>\n",
              "      <th>Acc C4</th>\n",
              "      <th>Acc C5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.086500</td>\n",
              "      <td>0.255354</td>\n",
              "      <td>0.908203</td>\n",
              "      <td>0.936250</td>\n",
              "      <td>0.939298</td>\n",
              "      <td>0.917388</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.902985</td>\n",
              "      <td>0.989796</td>\n",
              "      <td>0.969136</td>\n",
              "      <td>0.906897</td>\n",
              "      <td>0.906977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pruned dir: ./outputs_part2/BEST_PRUNED\n",
            "Pruned size(MB): 313.32\n",
            "Pruned metrics: {'eval_loss': 0.255354, 'eval_macro_f1': 0.908203, 'eval_accuracy': 0.93625, 'eval_balanced_accuracy': 0.939298, 'eval_kappa': 0.917388, 'eval_acc_c0': 0.96, 'eval_acc_c1': 0.902985, 'eval_acc_c2': 0.989796, 'eval_acc_c3': 0.969136, 'eval_acc_c4': 0.906897, 'eval_acc_c5': 0.906977, 'eval_runtime': 1.6796, 'eval_samples_per_second': 1428.891, 'eval_steps_per_second': 89.306, 'epoch': 1.0}\n",
            "\n",
            "=== COMPRESSION 2: DYNAMIC INT8 QUANTIZATION (CPU) ===\n",
            "Quant dir: ./outputs_part2/BEST_INT8_CPU\n",
            "Quant size(MB): 190.14\n",
            "Quant metrics: {'macro_f1': 0.904116, 'accuracy': 0.93125, 'balanced_accuracy': 0.931705, 'kappa': 0.91083, 'acc_c0': 0.954286, 'acc_c1': 0.902985, 'acc_c2': 0.969388, 'acc_c3': 0.947531, 'acc_c4': 0.92069, 'acc_c5': 0.895349}\n",
            "\n",
            "=== DONE ===\n",
            "Summary: ./outputs_part2/summary.json\n",
            "All outputs in: ./outputs_part2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    cohen_kappa_score,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--train_csv\", type=str, default=\"data/train.csv\")\n",
        "    p.add_argument(\"--text_col\", type=str, default=\"text\")\n",
        "    p.add_argument(\"--label_col\", type=str, default=\"label\")\n",
        "    p.add_argument(\"--out_dir\", type=str, default=\"./outputs_part2\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "    p.add_argument(\"--val_ratio\", type=float, default=0.15)\n",
        "    p.add_argument(\"--epochs\", type=int, default=10)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=16)\n",
        "    p.add_argument(\"--lr\", type=float, default=2e-5)\n",
        "    p.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
        "    p.add_argument(\"--max_length\", type=int, default=128)\n",
        "    p.add_argument(\"--patience\", type=int, default=2)\n",
        "    p.add_argument(\"--no_weighted_loss\", action=\"store_true\")\n",
        "    p.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    p.add_argument(\"--show_plots\", action=\"store_true\")\n",
        "\n",
        "    p.add_argument(\n",
        "        \"--supersample_factors\",\n",
        "        type=str,\n",
        "        default=\"1,1,4,2,3,8\",\n",
        "        help='e.g. \"1,1,1,1,1,1\" (no oversample) or \"1,1,1,1,1,2\" (double class 5)'\n",
        "    )\n",
        "\n",
        "    # Model Compression\n",
        "    p.add_argument(\"--prune_amount\", type=float, default=0.35)\n",
        "    p.add_argument(\"--prune_recover_epochs\", type=int, default=1)\n",
        "\n",
        "    args, _ = p.parse_known_args(argv)\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "\n",
        "    df = pd.read_csv(args.train_csv)\n",
        "    num_labels = int(df[args.label_col].nunique())\n",
        "\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        test_size=args.val_ratio,\n",
        "        random_state=args.seed,\n",
        "        stratify=df[args.label_col],\n",
        "    )\n",
        "\n",
        "    # Apply supersampling ONLY to training split\n",
        "    if args.supersample_factors is not None:\n",
        "        factors = parse_int_list(args.supersample_factors)\n",
        "        train_df = supersample_train_df(train_df, args.label_col, factors=factors, seed=args.seed)\n",
        "        print(\"Applied supersampling factors:\", factors)\n",
        "        print(\"New train label counts:\\n\", train_df[args.label_col].value_counts().sort_index())\n",
        "\n",
        "    candidates = [\n",
        "        \"roberta-base\",\n",
        "        \"cardiffnlp/twitter-roberta-base\",\n",
        "        \"distilroberta-base\"]\n",
        "\n",
        "    results: List[TrainResult] = []\n",
        "    for m in candidates:\n",
        "        print(f\"\\n=== Training {m} ===\")\n",
        "        r = train_one_model(\n",
        "            model_name=m,\n",
        "            train_df=train_df,\n",
        "            val_df=val_df,\n",
        "            text_col=args.text_col,\n",
        "            label_col=args.label_col,\n",
        "            num_labels=num_labels,\n",
        "            out_root=args.out_dir,\n",
        "            seed=args.seed,\n",
        "            epochs=args.epochs,\n",
        "            batch_size=args.batch_size,\n",
        "            lr=args.lr,\n",
        "            weight_decay=args.weight_decay,\n",
        "            max_length=args.max_length,\n",
        "            patience=args.patience,\n",
        "            use_weighted_loss=(not args.no_weighted_loss),\n",
        "            device=args.device,\n",
        "            show_plots=args.show_plots,\n",
        "        )\n",
        "        results.append(r)\n",
        "        print(\"Saved:\", r.saved_dir)\n",
        "        print(\"Loss plot:\", r.loss_plot_path)\n",
        "        print(\"Size(MB):\", round(r.size_mb, 2))\n",
        "        print(\"Eval macro-F1:\", round(r.best_metric, 6))\n",
        "\n",
        "    best = max(results, key=lambda x: x.best_metric)\n",
        "    print(\"\\n=== BEST MODEL ===\")\n",
        "    print(\"Best:\", best.model_name)\n",
        "    print(\"Dir :\", best.saved_dir)\n",
        "\n",
        "    # -------------------------\n",
        "    # COMPRESSION 1: PRUNING + RECOVERY FINETUNE\n",
        "    # -------------------------\n",
        "    print(\"\\n=== COMPRESSION 1: PRUNING + RECOVERY FINETUNE ===\")\n",
        "    best_tok = AutoTokenizer.from_pretrained(best.saved_dir, use_fast=True)\n",
        "    best_model = AutoModelForSequenceClassification.from_pretrained(best.saved_dir)\n",
        "\n",
        "    pruned_model = apply_global_magnitude_pruning(best_model, amount=args.prune_amount)\n",
        "    sp = linear_sparsity(pruned_model)\n",
        "    print(f\"Pruned linear sparsity: {sp:.3f}\")\n",
        "\n",
        "    pruned_dir = os.path.join(args.out_dir, \"BEST_PRUNED\")\n",
        "    os.makedirs(pruned_dir, exist_ok=True)\n",
        "\n",
        "    train_ds = CSVDataset(train_df, best_tok, args.text_col, args.label_col, max_length=args.max_length)\n",
        "    val_ds = CSVDataset(val_df, best_tok, args.text_col, args.label_col, max_length=args.max_length)\n",
        "    collator = DataCollatorWithPadding(best_tok)\n",
        "\n",
        "    class_counts = train_df[args.label_col].value_counts().sort_index().reindex(range(num_labels), fill_value=0).values\n",
        "    inv = 1.0 / torch.tensor(class_counts + 1e-9, dtype=torch.float)\n",
        "    class_weights = inv / inv.sum()\n",
        "\n",
        "    pruned_args = make_training_args(\n",
        "        output_dir=pruned_dir,\n",
        "        num_train_epochs=args.prune_recover_epochs,\n",
        "        learning_rate=min(args.lr, 1e-5),\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        greater_is_better=True,\n",
        "        logging_strategy=\"epoch\",\n",
        "        report_to=\"none\",\n",
        "        seed=args.seed,\n",
        "        fp16=(\"cuda\" in args.device and torch.cuda.is_available()),\n",
        "    )\n",
        "\n",
        "    if args.no_weighted_loss:\n",
        "        pruned_trainer = Trainer(\n",
        "            model=pruned_model,\n",
        "            args=pruned_args,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=val_ds,\n",
        "            tokenizer=best_tok,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "    else:\n",
        "        pruned_trainer = WeightedLossTrainer(\n",
        "            class_weights=class_weights,\n",
        "            model=pruned_model,\n",
        "            args=pruned_args,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=val_ds,\n",
        "            tokenizer=best_tok,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "    pruned_trainer.train()\n",
        "    pruned_metrics = pruned_trainer.evaluate()\n",
        "\n",
        "    pruned_trainer.save_model(pruned_dir)\n",
        "    best_tok.save_pretrained(pruned_dir)\n",
        "\n",
        "    pruned_loss_png = os.path.join(pruned_dir, \"loss_curve.png\")\n",
        "    plot_losses_from_trainer(pruned_trainer, pruned_loss_png, show=args.show_plots)\n",
        "\n",
        "    pruned_size = model_disk_size_mb(pruned_trainer.model)\n",
        "    print(\"Pruned dir:\", pruned_dir)\n",
        "    print(\"Pruned size(MB):\", round(pruned_size, 2))\n",
        "    print(\"Pruned metrics:\", {k: round(float(v), 6) for k, v in pruned_metrics.items() if isinstance(v, (int, float, np.floating))})\n",
        "\n",
        "    # -------------------------\n",
        "    # COMPRESSION 2: DYNAMIC INT8 QUANTIZATION (CPU)\n",
        "    # -------------------------\n",
        "    print(\"\\n=== COMPRESSION 2: DYNAMIC INT8 QUANTIZATION (CPU) ===\")\n",
        "    best_model_clean = AutoModelForSequenceClassification.from_pretrained(best.saved_dir)\n",
        "    qmodel = dynamic_int8_quantize(best_model_clean)\n",
        "\n",
        "    texts = val_df[args.text_col].astype(str).tolist()\n",
        "    labels = val_df[args.label_col].astype(int).to_numpy()\n",
        "\n",
        "    preds_all = []\n",
        "    bs = 32\n",
        "    for i in range(0, len(texts), bs):\n",
        "        batch = texts[i:i + bs]\n",
        "        enc = best_tok(batch, padding=True, truncation=True, max_length=args.max_length, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            logits = qmodel(**enc).logits\n",
        "        preds_all.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds_all, axis=0)\n",
        "\n",
        "    q_metrics = {\n",
        "        \"macro_f1\": float(f1_score(labels, preds, average=\"macro\")),\n",
        "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
        "        \"balanced_accuracy\": float(balanced_accuracy_score(labels, preds)),\n",
        "        \"kappa\": float(cohen_kappa_score(labels, preds)),\n",
        "    }\n",
        "    for c in range(num_labels):\n",
        "        mask = labels == c\n",
        "        q_metrics[f\"acc_c{c}\"] = float((preds[mask] == labels[mask]).mean()) if mask.sum() else float(\"nan\")\n",
        "\n",
        "    q_size = model_disk_size_mb(qmodel)\n",
        "\n",
        "    quant_dir = os.path.join(args.out_dir, \"BEST_INT8_CPU\")\n",
        "    os.makedirs(quant_dir, exist_ok=True)\n",
        "    torch.save(qmodel.state_dict(), os.path.join(quant_dir, \"pytorch_model.bin\"))\n",
        "    best_tok.save_pretrained(quant_dir)\n",
        "    with open(os.path.join(quant_dir, \"meta.json\"), \"w\") as f:\n",
        "        json.dump({\"base_model_dir\": best.saved_dir, \"note\": \"dynamic int8 quantized (CPU) Linear layers\"}, f, indent=2)\n",
        "\n",
        "    print(\"Quant dir:\", quant_dir)\n",
        "    print(\"Quant size(MB):\", round(q_size, 2))\n",
        "    print(\"Quant metrics:\", {k: round(float(v), 6) for k, v in q_metrics.items()})\n",
        "\n",
        "    # Summary JSON\n",
        "    summary_path = os.path.join(args.out_dir, \"summary.json\")\n",
        "    summary = {\n",
        "        \"best_model\": best.model_name,\n",
        "        \"best_dir\": best.saved_dir,\n",
        "        \"best_metrics\": best.metrics,\n",
        "        \"pruned_dir\": pruned_dir,\n",
        "        \"pruned_metrics\": {k: float(v) for k, v in pruned_metrics.items() if isinstance(v, (int, float, np.floating))},\n",
        "        \"pruned_sparsity\": sp,\n",
        "        \"quant_dir\": quant_dir,\n",
        "        \"quant_metrics\": q_metrics,\n",
        "    }\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(\"\\n=== DONE ===\")\n",
        "    print(\"Summary:\", summary_path)\n",
        "    print(\"All outputs in:\", args.out_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm-CgzuzcP3_"
      },
      "source": [
        "## Take a look at models outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgAEVmk0cR8v",
        "outputId": "b91cfb2d-f3be-4d90-a370-cba891586a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEST_INT8_CPU  cardiffnlp__twitter-roberta-base  roberta-base\n",
            "BEST_PRUNED    distilroberta-base\t\t summary.json\n"
          ]
        }
      ],
      "source": [
        "!ls outputs_part2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save"
      ],
      "metadata": {
        "id": "_HY3Q6zboOTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!mv outputs_part2 outputs_part2_1\n",
        "#!cp -r outputs_part2_1 gdrive/MyDrive/__PHd_2025/courses/2026a/NLP/."
      ],
      "metadata": {
        "id": "y5x3vN9zoPlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls outputs_part2_1 gdrive/MyDrive/__PHd_2025/courses/2026a/NLP/outputs_part2_1"
      ],
      "metadata": {
        "id": "SlHoFo39ojNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}